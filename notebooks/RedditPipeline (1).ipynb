{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Initialize a Spark Session. The App name refers to Reddit, since Reddit data is being processed. \n",
    "# The Spark master is located on host 8080, so this is denoted in the file.\n",
    "# The Session should be created if it doesn't exist yet, and otherwise get. \n",
    "spark = SparkSession.builder.appName(\"RedditData\").master(\"spark://spark-master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data will be read from the reddit_vm csv file and saved to df. Since the csv file uses a header, this will be denoted as true in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = spark.read.format(\"bigquery\").load(\"nodal-strength-325610.assignment2.reddit_data\")\n",
    "\n",
    "# Setup hadoop fs configuration\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\",\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\",\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopF5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+---------+-------------+--------------------+-------------------+\n",
      "|               title|score|    id|                 url|comms_num|      created|                body|          timestamp|\n",
      "+--------------------+-----+------+--------------------+---------+-------------+--------------------+-------------------+\n",
      "|FACT: vaccines ca...|    0|mhma6j|https://www.reddi...|       10|1.617246485E9|You guys don't kn...|2021-04-01 06:08:05|\n",
      "|Are vaccines made...|    0|f05tlq|https://www.reddi...|       15|1.581052672E9|Is it more likely...|2020-02-07 07:17:52|\n",
      "|Saying vaccines c...|    0|cb0ebr|https://www.reddi...|       22|  1.5626763E9|It may or may not...|2019-07-09 15:45:00|\n",
      "|I almost died of ...|    0|bu2j8m|https://www.reddi...|       15|1.559063033E9|my mom gave me a ...|2019-05-28 20:03:53|\n",
      "|Vacvimes cause Au...|    0|bigdzp|https://www.reddi...|       25|1.556487377E9|Everyone on this ...|2019-04-29 00:36:17|\n",
      "+--------------------+-----+------+--------------------+---------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "Next, some exploratory data analysis will be performed to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reddit posts: 1597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reddit posts: {}\\n\".format(reddit_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check if there are are posts without any text in the body. These cannot be used for sentiment analysis, and should be removed in the data cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+---------+-------------+--------------------+-------------------+\n",
      "|               title|score|    id|                 url|comms_num|      created|                body|          timestamp|\n",
      "+--------------------+-----+------+--------------------+---------+-------------+--------------------+-------------------+\n",
      "|FACT: vaccines ca...|    0|mhma6j|https://www.reddi...|       10|1.617246485E9|You guys don't kn...|2021-04-01 06:08:05|\n",
      "|Are vaccines made...|    0|f05tlq|https://www.reddi...|       15|1.581052672E9|Is it more likely...|2020-02-07 07:17:52|\n",
      "|Saying vaccines c...|    0|cb0ebr|https://www.reddi...|       22|  1.5626763E9|It may or may not...|2019-07-09 15:45:00|\n",
      "|I almost died of ...|    0|bu2j8m|https://www.reddi...|       15|1.559063033E9|my mom gave me a ...|2019-05-28 20:03:53|\n",
      "|Vacvimes cause Au...|    0|bigdzp|https://www.reddi...|       25|1.556487377E9|Everyone on this ...|2019-04-29 00:36:17|\n",
      "|Wait Wakefield sa...|    0|bh5d1p|https://www.reddi...|       32|1.556173612E9|Can't wait to see...|2019-04-25 09:26:52|\n",
      "|Myth- \"There is n...|    0|b3twz4|https://www.reddi...|      166|1.553192038E9|and yet...\n",
      "https:...|2019-03-21 20:13:58|\n",
      "|Mandatory Vaccina...|    0|azmxh6|https://www.reddi...|       39|1.552266113E9|The Association o...|2019-03-11 03:01:53|\n",
      "|Herd Immunity is ...|    0|azlm88|https://www.reddi...|       17|1.552258302E9|\n",
      "In 1933, Dr. Art...|2019-03-11 00:51:42|\n",
      "|Vaccine making yo...|    1|pjqjom|https://www.reddi...|       13|1.631032235E9|Ok so a worker at...|2021-09-07 19:30:35|\n",
      "|                 Vax|    1|aj95zn|https://www.reddi...|       34|1.548307332E9|So if your tellin...|2019-01-24 07:22:12|\n",
      "|5 Products you sh...|    2|afvif8|https://www.reddi...|       12|1.547470276E9|Vaccine products ...|2019-01-14 14:51:16|\n",
      "|No Heath Care and...|    3|myctlr|https://www.reddi...|       13|1.619371143E9|They seem to be p...|2021-04-25 20:19:03|\n",
      "|Do live vaccines ...|    5|e229g6|https://www.reddi...|       10|1.574793494E9|After you get a l...|2019-11-26 20:38:14|\n",
      "|Magnetic Therapy ...|    5|mxn4an|https://www.reddi...|       19|1.619280929E9|Hey all\n",
      "\n",
      "I was wo...|2021-04-24 19:15:29|\n",
      "|Need help with an...|    5|dtu0rj|https://www.reddi...|       32|1.573292979E9|Hi all.\n",
      "\n",
      "They’re ...|2019-11-09 11:49:39|\n",
      "|Any studies on ho...|    5|doyqqk|https://www.reddi...|       24|1.572393838E9|I'm compiling evi...|2019-10-30 02:03:58|\n",
      "|Is my daughter safe?|    7|fmsids|https://www.reddi...|       21|1.584845288E9|This might be a s...|2020-03-22 04:48:08|\n",
      "|    Anti-Vax Nurse?!|    7|dwwffd|https://www.reddi...|       27|1.573849752E9|Sorry if this is ...|2019-11-15 22:29:12|\n",
      "|Myth: 1 in 1000 m...|    8|b8jhk7|https://www.reddi...|       32|1.554213095E9|The 1 in 1000 fat...|2019-04-02 16:51:35|\n",
      "+--------------------+-----+------+--------------------+---------+-------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_data.filter(\"body is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reddit posts that have null in the body: 374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reddit posts that have null in the body: {}\\n\".format(reddit_data.filter(\"body is null\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "First, the rows with null values for the body will be removed. Next, unnecessary columns will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = reddit_data.filter(\"body is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_data = reddit_data.drop('timestamp')\n",
    "reddit_data = reddit_data.drop('comms_num')\n",
    "reddit_data = reddit_data.drop('url')\n",
    "reddit_data = reddit_data.drop('created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a special dataframe with only the text in the body, to perform sentiment analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_text = reddit_data.select('body','timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an index to the body text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "reddit_data_text = reddit_data_text.select(\"*\").withColumn(\"id\", monotonically_increasing_id()).select(\"id\",\"body\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|                body|          timestamp|\n",
      "+---+--------------------+-------------------+\n",
      "|  0|You guys don't kn...|2021-04-01 06:08:05|\n",
      "|  1|Is it more likely...|2020-02-07 07:17:52|\n",
      "|  2|It may or may not...|2019-07-09 15:45:00|\n",
      "|  3|my mom gave me a ...|2019-05-28 20:03:53|\n",
      "|  4|Everyone on this ...|2019-04-29 00:36:17|\n",
      "+---+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_data_text.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.9/site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.61.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+------------+\n",
      "| id|                body|          timestamp|   sentiment|\n",
      "+---+--------------------+-------------------+------------+\n",
      "|  0|You guys don't kn...|2021-04-01 06:08:05|        -0.3|\n",
      "|  1|Is it more likely...|2020-02-07 07:17:52|-0.060714286|\n",
      "|  2|It may or may not...|2019-07-09 15:45:00|-0.084821425|\n",
      "|  3|my mom gave me a ...|2019-05-28 20:03:53|        -0.8|\n",
      "|  4|Everyone on this ...|2019-04-29 00:36:17| -0.17045455|\n",
      "|  5|Can't wait to see...|2019-04-25 09:26:52|  0.10052632|\n",
      "|  6|and yet...\n",
      "https:...|2019-03-21 20:13:58|         0.5|\n",
      "|  7|The Association o...|2019-03-11 03:01:53|  0.14444445|\n",
      "|  8|\n",
      "In 1933, Dr. Art...|2019-03-11 00:51:42| 0.018333333|\n",
      "|  9|Ok so a worker at...|2021-09-07 19:30:35|        0.13|\n",
      "+---+--------------------+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There was an issue with the texblob module such that it did not allow for application of an udf to an entire column. \n",
    "# To fix this, we converted the pyspark df to a pandas df and simply appended a list of sentiments to the df.\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "pandas_df = reddit_data_text.toPandas()\n",
    "sentiment_list = list()\n",
    "\n",
    "for index, row in pandas_df.iterrows():\n",
    "    sentiment = row['body']\n",
    "    sentiment_list.append(TextBlob(sentiment).sentiment[0])\n",
    "\n",
    "sentiments_df = spark.createDataFrame(sentiment_list, FloatType()) \\\n",
    "                    .select(\"*\") \\\n",
    "                    .withColumn(\"id\", monotonically_increasing_id()) \n",
    "\n",
    "# Join dataframes\n",
    "reddit_sentiments = reddit_data_text.join(sentiments_df, \"id\") \\\n",
    "                    .withColumnRenamed(\"value\",\"sentiment\")\n",
    "# Peak\n",
    "reddit_sentiments.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and binning the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+------------+-----+\n",
      "|year|month|   average_sentiment|record_count|  bin|\n",
      "+----+-----+--------------------+------------+-----+\n",
      "|2019|   10| 0.12503105815913942|          36| 0.25|\n",
      "|2014|    4| 0.12171957641839981|           1| 0.25|\n",
      "|2020|    6|                 0.0|           1|    0|\n",
      "|2019|    5|-0.00840922754723...|          10|-0.25|\n",
      "|2021|    8| 0.08750000223517418|           4| 0.25|\n",
      "|2021|    6|                 0.0|           1|    0|\n",
      "|2019|    3|0.021486500636316262|          26| 0.25|\n",
      "|2021|    5|0.016446514447268686|          19| 0.25|\n",
      "|2021|   10|  0.5767857134342194|           2| 0.75|\n",
      "|2020|    3|0.012194234589558272|          42| 0.25|\n",
      "|2019|    8| 0.08687666524201632|          18| 0.25|\n",
      "|2021|   11|-0.31874999962747097|           4|-0.25|\n",
      "|2019|    6| 0.15000000099341074|           3| 0.25|\n",
      "|2021|    9|-0.04941359721124172|          12|-0.25|\n",
      "|2019|    1| -0.0360704114039739|           3|-0.25|\n",
      "|2019|    2|-0.07155612111091614|           1|-0.25|\n",
      "|2020|   12| 0.07246644298235576|           9| 0.25|\n",
      "|2021|    7| -0.1277721098491124|          14|-0.25|\n",
      "|2020|    4|-0.08333333432674409|           5|-0.25|\n",
      "|2021|    3| 0.04635935952620847|          35| 0.25|\n",
      "+----+-----+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping the average sentiments by month and year to get a better and more consise, aggregated overview\n",
    "\n",
    "reddit_grouped_sentiments = reddit_sentiments.groupBy(year(\"timestamp\"),month(\"timestamp\")) \\\n",
    "                                .agg(avg(\"sentiment\"), count(\"*\"))\n",
    "\n",
    "# Creating bins, here 0 is neutral, <0 is negative and >0 is positive. This will help visualisation.\n",
    "def categorizer(s):\n",
    "    if s == -1:\n",
    "        return \"-1\"\n",
    "    elif -1 < s <= -0.5:\n",
    "        return \"-0.75\"\n",
    "    elif -0.5 < s < 0:\n",
    "        return \"-0.25\"\n",
    "    elif s == 0:\n",
    "        return \"0\"\n",
    "    elif 0 < s <0.5:\n",
    "        return \"0.25\"\n",
    "    elif 0.5<= s <1:\n",
    "        return \"0.75\"\n",
    "    else:\n",
    "        return \"1\"\n",
    "        \n",
    "bin_udf = udf(categorizer, StringType() )\n",
    "bin_df = reddit_grouped_sentiments.withColumn(\"bin\", bin_udf(\"avg(sentiment)\"))\n",
    "reddit_final_sentiment_df = bin_df.select(\"*\") \\\n",
    "                                .withColumnRenamed(\"year(timestamp)\",\"year\") \\\n",
    "                                .withColumnRenamed(\"month(timestamp)\",\"month\") \\\n",
    "                                .withColumnRenamed(\"count(1)\",\"record_count\") \\\n",
    "                                .withColumnRenamed(\"avg(sentiment)\",\"average_sentiment\") \n",
    "reddit_final_sentiment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to save the sentiment analysis df to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export \n",
    "bucket = \"elise_ass2_temp\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "reddit_final_sentiment_df.write.format('bigquery') \\\n",
    "  .option('table', 'nodal-strength-325610.assignment2.reddit_sentiment_v2') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|      people|  251|\n",
      "|     measles|  175|\n",
      "|       study|  143|\n",
      "|      autism|  139|\n",
      "|    children|  109|\n",
      "|       don’t|   87|\n",
      "|      immune|   83|\n",
      "|    immunity|   83|\n",
      "|     mercury|   82|\n",
      "|        time|   68|\n",
      "|         mmr|   68|\n",
      "|         cdc|   68|\n",
      "|unvaccinated|   67|\n",
      "|         flu|   65|\n",
      "|        kids|   62|\n",
      "|        risk|   61|\n",
      "|     studies|   60|\n",
      "|       years|   59|\n",
      "|      health|   59|\n",
      "|     medical|   58|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(inputCol='body', outputCol='body_tokenized')\n",
    "reddit_data_text_tokenized = tokenizer.transform(reddit_data_text).select('id', 'body_tokenized')\n",
    "\n",
    "# Remove unnecessary words\n",
    "unncessary_words_list = [\"like\", \"know\", \"get\", 'one', 'think', 'cause', 'say', 'even', \"don't\", 'got', 'also', 'good', 'said',\n",
    "                        'make', 'it.', 'first', 'many', 'still', 'actually', \"don't\", 'want', 'read', 'print', 'vaccine',\n",
    "                        'vaccines', 'vaccinated', 'vaccination', 'may', 'saying', 'point', 'virus', 'never', 'much', 'see',\n",
    "                        '1', 'way', 'wrong', 'really', 'used', 'well', 'getting', 'take', 'every', 'go', '>'] \n",
    "unncessary_words_list.extend(StopWordsRemover().getStopWords())\n",
    "remover = StopWordsRemover(inputCol='body_tokenized', outputCol='body_clean', stopWords=unncessary_words_list)\n",
    "reddit_data_text_no_stopwords = remover.transform(reddit_data_text_tokenized).select(['id', 'body_clean'])\n",
    "\n",
    "# Return to regular strings\n",
    "reddit_data_text_no_stopwords = reddit_data_text_no_stopwords.withColumn(\"body_clean\", \n",
    "                                                                         concat_ws(\",\", \"body_clean\"))\n",
    "\n",
    "# Count the words\n",
    "reddit_count_df = reddit_data_text_no_stopwords.withColumn('body_clean', f.explode(f.split(f.col('body_clean'), ',')))\\\n",
    "    .groupBy('body_clean')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .filter(f.col('body_clean') != \"\")\\\n",
    "    .withColumnRenamed(\"body_clean\",\"word\")\n",
    "\n",
    "reddit_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word count\n",
    "reddit_count_df.write.format('bigquery') \\\n",
    "  .option('table', 'nodal-strength-325610.assignment2.reddit_word_count2') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the spark context should be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
