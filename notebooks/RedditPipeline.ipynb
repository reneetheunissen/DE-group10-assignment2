{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Initialize a Spark Session. The App name refers to Reddit, since Reddit data is being processed. \n",
    "# The Spark master is located on host 8080, so this is denoted in the file.\n",
    "# The Session should be created if it doesn't exist yet, and otherwise get. \n",
    "spark = SparkSession.builder.appName(\"RedditData\").master(\"spark://spark-master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data will be read from the reddit_vm csv file and saved to df. Since the csv file uses a header, this will be denoted as true in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = spark.read.format(\"bigquery\").load(\"de2021labs.reddit_data.reddit_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+----+---------+-------------+--------------------+-------------------+\n",
      "|  title|score|     id| url|comms_num|      created|                body|          timestamp|\n",
      "+-------+-----+-------+----+---------+-------------+--------------------+-------------------+\n",
      "|Comment|    0|fofa0yy|null|        0| 1.58775959E9|Because Anti-Vaxx...|2020-04-24 23:19:50|\n",
      "|Comment|    0|ej9xuaf|null|        0|1.553474721E9|What do you mean ...|2019-03-25 02:45:21|\n",
      "|Comment|    0|ejc2imz|null|        0|1.553547167E9|Good thing I didn...|2019-03-25 22:52:47|\n",
      "|Comment|    0|ejah7aa|null|        0| 1.55348913E9|What is thimerosa...|2019-03-25 06:45:30|\n",
      "|Comment|    0|ejagfa0|null|        0|1.553488573E9|I never once said...|2019-03-25 06:36:13|\n",
      "+-------+-----+-------+----+---------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "Next, some exploratory data analysis will be performed to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reddit posts: 1597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reddit posts: {}\\n\".format(reddit_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check if there are are posts without any text in the body. These cannot be used for sentiment analysis, and should be removed in the data cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+----+---------+-------------+--------------------+-------------------+\n",
      "|  title|score|     id| url|comms_num|      created|                body|          timestamp|\n",
      "+-------+-----+-------+----+---------+-------------+--------------------+-------------------+\n",
      "|Comment|    0|fofa0yy|null|        0| 1.58775959E9|Because Anti-Vaxx...|2020-04-24 23:19:50|\n",
      "|Comment|    0|ej9xuaf|null|        0|1.553474721E9|What do you mean ...|2019-03-25 02:45:21|\n",
      "|Comment|    0|ejc2imz|null|        0|1.553547167E9|Good thing I didn...|2019-03-25 22:52:47|\n",
      "|Comment|    0|ejah7aa|null|        0| 1.55348913E9|What is thimerosa...|2019-03-25 06:45:30|\n",
      "|Comment|    0|ejagfa0|null|        0|1.553488573E9|I never once said...|2019-03-25 06:36:13|\n",
      "|Comment|    0|ejaftc8|null|        0|1.553488143E9|Sodium and chloride.|2019-03-25 06:29:03|\n",
      "|Comment|    0|ek76jw5|null|        0|1.554521745E9|Why are you givin...|2019-04-06 06:35:45|\n",
      "|Comment|    0|ek49q2s|null|        0|1.554430197E9|The problem is th...|2019-04-05 05:09:57|\n",
      "|Comment|    0|ek3pc0s|null|        0|1.554416607E9|In this instance,...|2019-04-05 01:23:27|\n",
      "|Comment|    0|el2ppps|null|        0|1.555499865E9|Anti-vax is [prom...|2019-04-17 14:17:45|\n",
      "|Comment|    0|elqybv1|null|        0|1.556202798E9|     Excellent. Why?|2019-04-25 17:33:18|\n",
      "|Comment|    0|hce41j3|null|        0| 1.63132825E9|Can I get that li...|2021-09-11 05:44:10|\n",
      "|Comment|    0|gy4xnwb|null|        0|1.621021185E9|I have to dm you ...|2021-05-14 22:39:45|\n",
      "|Comment|    0|gy46z6k|null|        0|1.621009604E9|Gene editing expe...|2021-05-14 19:26:44|\n",
      "|Comment|    0|gy44579|null|        0|1.621008398E9|YOU. ARE. DUMB. A...|2021-05-14 19:06:38|\n",
      "|Comment|    0|gy43ano|null|        0|1.621008029E9|I knew that you l...|2021-05-14 19:00:29|\n",
      "|Comment|    0|gy3agnc|null|        0| 1.62099463E9|You realize more ...|2021-05-14 15:17:10|\n",
      "|Comment|    0|gwnwdt4|null|        0|1.619964788E9|Dude, you are suc...|2021-05-02 17:13:08|\n",
      "|Comment|    0|gvxm3im|null|        0|1.619447597E9|Most of this comm...|2021-04-26 17:33:17|\n",
      "|Comment|    0|gvwoomp|null|        0|1.619423915E9|>Some states have...|2021-04-26 10:58:35|\n",
      "+-------+-----+-------+----+---------+-------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_data.filter(\"body is not null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reddit posts that have null in the body: 374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reddit posts that have null in the body: {}\\n\".format(reddit_data.filter(\"body is null\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "First, the rows with null values for the body will be removed. Next, unnecessary columns will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = reddit_data.filter(\"body is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = reddit_data.drop('comms_num')\n",
    "reddit_data = reddit_data.drop('url')\n",
    "reddit_data = reddit_data.drop('created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a special dataframe with only the text in the body, to perform sentiment analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_text = reddit_data.select(['body', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an index to the body text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "reddit_data_text = reddit_data_text.select(\"*\").withColumn(\"id\", monotonically_increasing_id()).select(\"id\",\"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                body|\n",
      "+---+--------------------+\n",
      "|  0|Because Anti-Vaxx...|\n",
      "|  1|What do you mean ...|\n",
      "|  2|Good thing I didn...|\n",
      "|  3|What is thimerosa...|\n",
      "|  4|I never once said...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_data_text.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/conda/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/conda/lib/python3.9/site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2021.11.10)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.61.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                body|               value|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Because Anti-Vaxx...|                 0.0|\n",
      "|  1|What do you mean ...|-0.08806818181818182|\n",
      "|  2|Good thing I didn...| 0.13333333333333333|\n",
      "|  3|What is thimerosa...|                 0.0|\n",
      "|  4|I never once said...|-0.19166666666666662|\n",
      "|  5|Sodium and chloride.|                 0.0|\n",
      "|  6|Why are you givin...| 0.10570779220779221|\n",
      "|  7|The problem is th...|-0.01515151515151...|\n",
      "|  8|In this instance,...|                 0.0|\n",
      "|  9|Anti-vax is [prom...| 0.19999999999999998|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There was an issue with the texblob module such that it did not allow for application of an udf to an entire column. \n",
    "# To fix this, we converted the pyspark df to a pandas df and simply appended a list of sentiments to the df.\n",
    "\n",
    "pandas_df = reddit_data_text.toPandas()\n",
    "sentiment_list = list()\n",
    "\n",
    "for index, row in pandas_df.iterrows():\n",
    "    sentiment = row['body']\n",
    "    sentiment_list.append(TextBlob(sentiment).sentiment[0])\n",
    "\n",
    "sentiments_df = spark.createDataFrame(sentiment_list, StringType()) \\\n",
    "                    .select(\"*\") \\\n",
    "                    .withColumn(\"id\", monotonically_increasing_id()) \n",
    "\n",
    "# Join dataframes\n",
    "reddit_sentiments = reddit_data_text.join(sentiments_df, \"id\")\n",
    "\n",
    "# Peak\n",
    "reddit_sentiments.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to save the sentiment analysis df to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"dejadsdejads_group10assignment2\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "reddit_sentiments.write.format('bigquery') \\\n",
    "  .option('table', 'de2021labs.reddit_data.reddit_sentiments') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word count\n",
    "Besides sentiment analysis, a word count is also performed for the reddit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(inputCol='body', outputCol='body_tokenized')\n",
    "reddit_data_text_tokenized = tokenizer.transform(reddit_data_text).select('id', 'body_tokenized')\n",
    "\n",
    "# Remove unnecessary words\n",
    "unncessary_words_list = [\"like\", \"know\", \"get\", 'one', 'think', 'cause', 'say', 'even', \"don't\", 'got', 'also', 'good', 'said',\n",
    "                        'make', 'it.', 'first', 'many', 'still', 'actually', \"don't\", 'want', 'read', 'print', 'vaccine',\n",
    "                        'vaccines', 'vaccinated', 'vaccination', 'may', 'saying', 'point', 'virus', 'never', 'much', 'see',\n",
    "                        '1', 'way', 'wrong', 'really', 'used', 'well', 'getting', 'take', 'every', 'go', '>'] \n",
    "unncessary_words_list.extend(StopWordsRemover().getStopWords())\n",
    "remover = StopWordsRemover(inputCol='body_tokenized', outputCol='body_clean', stopWords=unncessary_words_list)\n",
    "reddit_data_text_no_stopwords = remover.transform(reddit_data_text_tokenized).select(['id', 'body_clean'])\n",
    "\n",
    "# Return to regular strings\n",
    "reddit_data_text_no_stopwords = reddit_data_text_no_stopwords.withColumn(\"body_clean\", \n",
    "                                                                         concat_ws(\",\", \"body_clean\"))\n",
    "\n",
    "# Count the words\n",
    "reddit_data_text_no_stopwords.withColumn('body_clean', f.explode(f.split(f.col('body_clean'), ',')))\\\n",
    "    .groupBy('body_clean')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .filter(f.col('body_clean') != \"\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word count\n",
    "bucket = \"dejadsdejads_group10assignment2\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "reddit_sentiments.write.format('bigquery') \\\n",
    "  .option('table', 'de2021labs.reddit_data.reddit_wordcount') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, the spark context should be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
